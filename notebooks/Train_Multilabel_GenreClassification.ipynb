{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O7hSkaeVGnED",
        "6FIK_zV4fx_D"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEZP3Z0tC0TO",
        "outputId": "9f34227f-ca43-4e51-e830-e71d02afdd0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset via Kaggle commandline API\n",
        "!kaggle datasets download -d jorgeruizdev/ludwig-music-dataset-moods-and-subgenres\n",
        "!unzip ludwig-music-dataset-moods-and-subgenres.zip -d ludwig"
      ],
      "metadata": {
        "id": "VYQaCDNsDDOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MP3 to NPY (Mel-Spectrogram)\n",
        "**For this part, you must upload the train.csv and test.csv files from the GitHub Repository**"
      ],
      "metadata": {
        "id": "O7hSkaeVGnED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STR_CLIP_ID = 'clip_id'\n",
        "STR_AUDIO_SIGNAL = 'audio_signal'\n",
        "STR_TARGET_VECTOR = 'target_vector'\n",
        "\n",
        "\n",
        "STR_CH_FIRST = 'channels_first'\n",
        "STR_CH_LAST = 'channels_last'\n",
        "\n",
        "import io\n",
        "import os\n",
        "import tqdm\n",
        "import logging\n",
        "import subprocess\n",
        "from typing import Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "# import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "import itertools\n",
        "from numpy.fft import irfft\n",
        "\n",
        "def _resample_load_ffmpeg(path: str, sample_rate: int, downmix_to_mono: bool) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Decoding, downmixing, and downsampling by librosa.\n",
        "    Returns a channel-first audio signal.\n",
        "\n",
        "    Args:\n",
        "        path:\n",
        "        sample_rate:\n",
        "        downmix_to_mono:\n",
        "\n",
        "    Returns:\n",
        "        (audio signal, sample rate)\n",
        "    \"\"\"\n",
        "\n",
        "    def _decode_resample_by_ffmpeg(filename, sr):\n",
        "        \"\"\"decode, downmix, and resample audio file\"\"\"\n",
        "        channel_cmd = '-ac 1 ' if downmix_to_mono else ''  # downmixing option\n",
        "        resampling_cmd = f'-ar {str(sr)}' if sr else ''  # downsampling option\n",
        "        cmd = f\"ffmpeg -i \\\"{filename}\\\" {channel_cmd} {resampling_cmd} -f wav -\"\n",
        "        p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        out, err = p.communicate()\n",
        "        return out\n",
        "\n",
        "    src, sr = sf.read(io.BytesIO(_decode_resample_by_ffmpeg(path, sr=sample_rate)))\n",
        "    return src.T, sr\n",
        "\n",
        "\n",
        "def _resample_load_librosa(path: str, sample_rate: int, downmix_to_mono: bool, **kwargs) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Decoding, downmixing, and downsampling by librosa.\n",
        "    Returns a channel-first audio signal.\n",
        "    \"\"\"\n",
        "    src, sr = librosa.load(path, sr=sample_rate, mono=downmix_to_mono, **kwargs)\n",
        "    return src, sr\n",
        "\n",
        "\n",
        "def load_audio(\n",
        "    path: str or Path,\n",
        "    ch_format: str,\n",
        "    sample_rate: int = None,\n",
        "    downmix_to_mono: bool = False,\n",
        "    resample_by: str = 'ffmpeg',\n",
        "    **kwargs,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"A wrapper of librosa.load that:\n",
        "        - forces the returned audio to be 2-dim,\n",
        "        - defaults to sr=None, and\n",
        "        - defaults to downmix_to_mono=False.\n",
        "\n",
        "    The audio decoding is done by `audioread` or `soundfile` package and ultimately, often by ffmpeg.\n",
        "    The resampling is done by `librosa`'s child package `resampy`.\n",
        "\n",
        "    Args:\n",
        "        path: audio file path\n",
        "        ch_format: one of 'channels_first' or 'channels_last'\n",
        "        sample_rate: target sampling rate. if None, use the rate of the audio file\n",
        "        downmix_to_mono:\n",
        "        resample_by (str): 'librosa' or 'ffmpeg'. it decides backend for audio decoding and resampling.\n",
        "        **kwargs: keyword args for librosa.load - offset, duration, dtype, res_type.\n",
        "\n",
        "    Returns:\n",
        "        (audio, sr) tuple\n",
        "    \"\"\"\n",
        "    if ch_format not in (STR_CH_FIRST, STR_CH_LAST):\n",
        "        raise ValueError(f'ch_format is wrong here -> {ch_format}')\n",
        "\n",
        "    if os.stat(path).st_size > 8000:\n",
        "        if resample_by == 'librosa':\n",
        "            src, sr = _resample_load_librosa(path, sample_rate, downmix_to_mono, **kwargs)\n",
        "        elif resample_by == 'ffmpeg':\n",
        "            src, sr = _resample_load_ffmpeg(path, sample_rate, downmix_to_mono)\n",
        "        else:\n",
        "            raise NotImplementedError(f'resample_by: \"{resample_by}\" is not supposred yet')\n",
        "    else:\n",
        "        raise ValueError('Given audio is too short!')\n",
        "    return src, sr"
      ],
      "metadata": {
        "id": "_Qfetl0xGvUb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code modified and adapted from https://github.com/seungheondoh/lp-music-caps/blob/main/lpmc/music_captioning/preprocessor.py\n",
        "\"\"\"\n",
        "import os\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import csv\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "# hard coding hparams\n",
        "DATASET_PATH = \"/content/ludwig\"\n",
        "MUSIC_SAMPLE_RATE = 16000 # resampling rate of MERT\n",
        "DURATION = 30 # whisper expected input length\n",
        "DATA_LENGTH = int(MUSIC_SAMPLE_RATE * DURATION)\n",
        "\n",
        "def get_all_audio_paths():\n",
        "    # Directory where the mp3 files are stored\n",
        "    root_dir = DATASET_PATH + \"/mp3/mp3/\"\n",
        "\n",
        "    # This list will hold all the mp3 file paths\n",
        "    mp3_paths = []\n",
        "\n",
        "    # Walk through the directory and subdirectories\n",
        "    for genre_dir, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.mp3'):  # Filter for mp3 files\n",
        "                mp3_paths.append(os.path.join(genre_dir, file))\n",
        "    return mp3_paths\n",
        "\n",
        "def get_audio_paths(audio_csv):\n",
        "    test_paths = []\n",
        "    csv.field_size_limit(sys.maxsize)\n",
        "    with open(audio_csv, 'r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            test_paths.append(row['id'])\n",
        "    all_paths = get_all_audio_paths()\n",
        "    print(f\"Found {len(all_paths)} audio clips\")\n",
        "    audio_paths = [\n",
        "      path for path in all_paths\n",
        "      if path.rpartition('/')[-1].rpartition('.mp3')[0] in test_paths\n",
        "    ]\n",
        "    print(f\"Loading {len(audio_paths)} audio clips\")\n",
        "    return audio_paths\n",
        "\n",
        "def msd_resampler(sample):\n",
        "    path = sample\n",
        "    save_name = os.path.join(DATASET_PATH,'npy', path.rpartition('/')[-1].replace(\".mp3\",\".npy\"))\n",
        "    try:\n",
        "        src, _ = load_audio(\n",
        "            path=path,\n",
        "            ch_format= STR_CH_FIRST,\n",
        "            sample_rate= MUSIC_SAMPLE_RATE,\n",
        "            downmix_to_mono= True)\n",
        "    except ValueError as err:\n",
        "        print(f\"{err} for {sample}\")\n",
        "        return\n",
        "    if src.shape[-1] < DATA_LENGTH: # short case\n",
        "        pad = np.zeros(DATA_LENGTH)\n",
        "        pad[:src.shape[-1]] = src\n",
        "        src = pad\n",
        "    elif src.shape[-1] > DATA_LENGTH: # too long case\n",
        "        src = src[:DATA_LENGTH]\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(save_name)):\n",
        "        os.makedirs(os.path.dirname(save_name), exist_ok=True)\n",
        "    np.save(save_name, src.astype(np.float32))\n",
        "\n",
        "def main():\n",
        "    all_samples = get_audio_paths(\"train.csv\")\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        for _ in tqdm(pool.imap_unordered(msd_resampler, all_samples), total=len(all_samples)):\n",
        "            pass\n",
        "    print(\"finish extract\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAOWeibaGmQ7",
        "outputId": "9b77a8c2-168d-4699-a789-929795cb04f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11294 audio clips\n",
            "Loading 3268 audio clips\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3268/3268 [03:59<00:00, 13.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish extract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/ludwig/npy /content/ludwig/npy_train"
      ],
      "metadata": {
        "id": "dmA7fgb7QfZ6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    all_samples = get_audio_paths(\"test.csv\")\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        for _ in tqdm(pool.imap_unordered(msd_resampler, all_samples), total=len(all_samples)):\n",
        "            pass\n",
        "    print(\"finish extract\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQJnAQz0P9RH",
        "outputId": "e6ed41ec-c7ce-4b5e-9f4b-bc7fbfe41e09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11294 audio clips\n",
            "Loading 318 audio clips\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 318/318 [00:23<00:00, 13.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish extract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/ludwig/npy /content/ludwig/npy_test"
      ],
      "metadata": {
        "id": "6z3vadjbQimq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset, Linear Probing Model, and Training Utilities"
      ],
      "metadata": {
        "id": "IrpT7i3MbfXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchaudio.transforms as T\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Wav2Vec2Processor\n",
        "import torchaudio.transforms as T\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    http://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # Convert predictions and labels to tensors\n",
        "    logits, labels = p\n",
        "\n",
        "    # Apply sigmoid to logits to get probabilities\n",
        "    predictions = torch.sigmoid(torch.tensor(logits)).cpu().numpy()\n",
        "\n",
        "    # Binarize the predictions to get 0 or 1 for multilabel classification\n",
        "    predictions = (predictions > 0.3).astype(int)\n",
        "\n",
        "    hamming_loss_val = hamming_loss(labels, predictions)\n",
        "    hamming_score_val = hamming_score(labels, predictions)\n",
        "\n",
        "    return {'hamming_score': hamming_score_val, 'hamming_loss': hamming_loss_val}\n",
        "\n",
        "class LudwigDataset(Dataset):\n",
        "    def __init__(self, csv_path, npy_folder, processor, mlb=None, fit_mlb=False):\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Filter rows based on the presence of corresponding .npy files in the npy folder\n",
        "        valid_ids = [\n",
        "            id_ for id_ in df['id'].astype(str)\n",
        "            if os.path.isfile(os.path.join(npy_folder, f\"{id_}.npy\"))\n",
        "        ]\n",
        "        self.dataset = df[df['id'].astype(str).isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.npy_folder = npy_folder\n",
        "        self.processor = processor\n",
        "        self.resample_rate = processor.sampling_rate\n",
        "        print(f\"Dataset size: {len(self.dataset)}\")\n",
        "        print(f\"Sample rate: {self.resample_rate}\")\n",
        "\n",
        "        # One-hot encode subgenre labels\n",
        "        if mlb is None:\n",
        "            self.mlb = MultiLabelBinarizer()\n",
        "        else:\n",
        "            self.mlb = mlb\n",
        "\n",
        "        if fit_mlb:\n",
        "            self.labels = self.mlb.fit_transform(self.dataset['subgenres'].apply(eval))\n",
        "        else:\n",
        "            self.labels = self.mlb.transform(self.dataset['subgenres'].apply(eval))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the audio file from npy folder\n",
        "        row = self.dataset.iloc[idx]\n",
        "        audio_path = os.path.join(self.npy_folder, f\"{row['id']}.npy\")\n",
        "        audio = np.load(audio_path, mmap_mode='r')\n",
        "\n",
        "        # Load one-hot encoded labels\n",
        "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "        # Process audio\n",
        "        inputs = self.processor(audio, sampling_rate=self.resample_rate, return_tensors=\"pt\", padding=True)\n",
        "        inputs[\"labels\"] = labels\n",
        "\n",
        "        return {key: val.squeeze(0) for key, val in inputs.items()}\n"
      ],
      "metadata": {
        "id": "H064M5SOZz9C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# design class weights for imbalanced dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "npy_folder = \"/content/ludwig/npy_train\"\n",
        "\n",
        "# Filter rows based on the presence of corresponding .npy files in the npy folder\n",
        "valid_ids = [\n",
        "    id_ for id_ in df['id'].astype(str)\n",
        "    if os.path.isfile(os.path.join(npy_folder, f\"{id_}.npy\"))\n",
        "]\n",
        "dataset = df[df['id'].astype(str).isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "# One-hot encode subgenre labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(dataset['subgenres'].apply(eval))\n",
        "\n",
        "# Calculate class weights based on label frequencies\n",
        "label_counts = np.sum(labels, axis=0)  # Sum across all samples for each class\n",
        "print(f\"Class Counts:\\n{label_counts}\")\n",
        "total_samples = labels.shape[0]\n",
        "class_frequencies = label_counts / total_samples  # Frequency of each class (subgenre)\n",
        "\n",
        "# Inverse of the class frequencies to compute class weights\n",
        "class_weights = total_samples / (len(mlb.classes_) * label_counts)\n",
        "\n",
        "# Assign computed class weights to self.class_weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Class Weights:\\n{class_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWjVmsPDdHAh",
        "outputId": "55f8c3b6-0dc8-41e8-cbd7-86fa3df6f943"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Counts:\n",
            "[120 113   8  14  20   5  11  90 142 120  88 146 126 119 193 129 114 174\n",
            " 182 104 117  65 114  25  55  72 101  80  13  34  15   1  21  25 128  91\n",
            " 113  31 169 113 118 110 139 144 156 105 168  66  57 118 124 127  53]\n",
            "Class Weights:\n",
            "tensor([ 0.5138,  0.5457,  7.7075,  4.4043,  3.0830, 12.3321,  5.6055,  0.6851,\n",
            "         0.4342,  0.5138,  0.7007,  0.4223,  0.4894,  0.5182,  0.3195,  0.4780,\n",
            "         0.5409,  0.3544,  0.3388,  0.5929,  0.5270,  0.9486,  0.5409,  2.4664,\n",
            "         1.1211,  0.8564,  0.6105,  0.7708,  4.7431,  1.8135,  4.1107, 61.6604,\n",
            "         2.9362,  2.4664,  0.4817,  0.6776,  0.5457,  1.9890,  0.3649,  0.5457,\n",
            "         0.5225,  0.5605,  0.4436,  0.4282,  0.3953,  0.5872,  0.3670,  0.9342,\n",
            "         1.0818,  0.5225,  0.4973,  0.4855,  1.1634], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, Trainer, TrainingArguments, Wav2Vec2FeatureExtractor, AutoModelForAudioClassification\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "\n",
        "# Load DistilHuBERT processor and model\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"pedromatias97/genre-recognizer-finetuned-gtzan_dset\")\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"pedromatias97/genre-recognizer-finetuned-gtzan_dset\",\n",
        "    num_labels=53,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "# set problem type for HuggingFace config\n",
        "model.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "# print model layers\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)\n",
        "\n",
        "\"\"\"\n",
        "Layers:\n",
        "  - hubert\n",
        "  - projector (256 out)\n",
        "  - classifier (256 in, num_classes out)\n",
        "\"\"\"\n",
        "\n",
        "# Replace the classification head with a multilabel-compatible one\n",
        "model.classifier = nn.Linear(256, 53) # 256 comes from output dims of projector layer\n",
        "\n",
        "# Override the forward method to handle logits directly for BCEWithLogitsLoss\n",
        "def forward_with_loss(input_values, attention_mask=None, labels=None):\n",
        "    # feature extractor (hubert)\n",
        "    outputs = model.hubert(input_values, attention_mask=attention_mask)  # Using feature extractor from Hubert\n",
        "\n",
        "    # embeddings through the projector layer\n",
        "    projected_features = model.projector(outputs.last_hidden_state[:, 0, :])  # Assuming using CLS token for classification\n",
        "\n",
        "    #  projected features through the classifier\n",
        "    logits = model.classifier(projected_features)\n",
        "\n",
        "    # print(torch.sigmoid(logits))\n",
        "    if labels is not None:\n",
        "        loss = binary_cross_entropy_with_logits(logits, labels.float(), weight=class_weights)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "    return {\"logits\": logits}\n",
        "\n",
        "model.forward = forward_with_loss\n",
        "\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:0.01f}M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h1ujzyU3gM5",
        "outputId": "fd29f6b6-c9de-4dbc-f09c-6fb225fed14f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at pedromatias97/genre-recognizer-finetuned-gtzan_dset and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([10, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([53]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hubert.masked_spec_embed\n",
            "hubert.feature_extractor.conv_layers.0.conv.weight\n",
            "hubert.feature_extractor.conv_layers.0.layer_norm.weight\n",
            "hubert.feature_extractor.conv_layers.0.layer_norm.bias\n",
            "hubert.feature_extractor.conv_layers.1.conv.weight\n",
            "hubert.feature_extractor.conv_layers.2.conv.weight\n",
            "hubert.feature_extractor.conv_layers.3.conv.weight\n",
            "hubert.feature_extractor.conv_layers.4.conv.weight\n",
            "hubert.feature_extractor.conv_layers.5.conv.weight\n",
            "hubert.feature_extractor.conv_layers.6.conv.weight\n",
            "hubert.feature_projection.projection.weight\n",
            "hubert.feature_projection.projection.bias\n",
            "hubert.encoder.pos_conv_embed.conv.bias\n",
            "hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0\n",
            "hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1\n",
            "hubert.encoder.layer_norm.weight\n",
            "hubert.encoder.layer_norm.bias\n",
            "hubert.encoder.layers.0.attention.k_proj.weight\n",
            "hubert.encoder.layers.0.attention.k_proj.bias\n",
            "hubert.encoder.layers.0.attention.v_proj.weight\n",
            "hubert.encoder.layers.0.attention.v_proj.bias\n",
            "hubert.encoder.layers.0.attention.q_proj.weight\n",
            "hubert.encoder.layers.0.attention.q_proj.bias\n",
            "hubert.encoder.layers.0.attention.out_proj.weight\n",
            "hubert.encoder.layers.0.attention.out_proj.bias\n",
            "hubert.encoder.layers.0.layer_norm.weight\n",
            "hubert.encoder.layers.0.layer_norm.bias\n",
            "hubert.encoder.layers.0.feed_forward.intermediate_dense.weight\n",
            "hubert.encoder.layers.0.feed_forward.intermediate_dense.bias\n",
            "hubert.encoder.layers.0.feed_forward.output_dense.weight\n",
            "hubert.encoder.layers.0.feed_forward.output_dense.bias\n",
            "hubert.encoder.layers.0.final_layer_norm.weight\n",
            "hubert.encoder.layers.0.final_layer_norm.bias\n",
            "hubert.encoder.layers.1.attention.k_proj.weight\n",
            "hubert.encoder.layers.1.attention.k_proj.bias\n",
            "hubert.encoder.layers.1.attention.v_proj.weight\n",
            "hubert.encoder.layers.1.attention.v_proj.bias\n",
            "hubert.encoder.layers.1.attention.q_proj.weight\n",
            "hubert.encoder.layers.1.attention.q_proj.bias\n",
            "hubert.encoder.layers.1.attention.out_proj.weight\n",
            "hubert.encoder.layers.1.attention.out_proj.bias\n",
            "hubert.encoder.layers.1.layer_norm.weight\n",
            "hubert.encoder.layers.1.layer_norm.bias\n",
            "hubert.encoder.layers.1.feed_forward.intermediate_dense.weight\n",
            "hubert.encoder.layers.1.feed_forward.intermediate_dense.bias\n",
            "hubert.encoder.layers.1.feed_forward.output_dense.weight\n",
            "hubert.encoder.layers.1.feed_forward.output_dense.bias\n",
            "hubert.encoder.layers.1.final_layer_norm.weight\n",
            "hubert.encoder.layers.1.final_layer_norm.bias\n",
            "projector.weight\n",
            "projector.bias\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "Model Parameters: 23.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train MERT on Multi-label Audio Clips"
      ],
      "metadata": {
        "id": "NnaY1aTJbS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Prepare dataset and dataloaders\n",
        "train_dataset = LudwigDataset(\n",
        "    csv_path='train.csv',\n",
        "    npy_folder='/content/ludwig/npy_train',\n",
        "    processor=processor,\n",
        "    fit_mlb=True  # Fit the MultiLabelBinarizer on the training data\n",
        ")\n",
        "\n",
        "mlb = train_dataset.mlb\n",
        "\n",
        "val_dataset = LudwigDataset(\n",
        "    csv_path='test.csv',\n",
        "    npy_folder='/content/ludwig/npy_test',\n",
        "    processor=processor,\n",
        "    mlb=mlb,  # Use the fitted MultiLabelBinarizer\n",
        "    fit_mlb=False  # Do not refit on test data\n",
        ")\n",
        "\n",
        "# Instantiate the model\n",
        "# model = GenreMultilabelModel(model, num_classes=len(train_dataset.mlb.classes_))\n",
        "print(f\"Number of classes: {len(train_dataset.mlb.classes_)}\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",  # Evaluate based on steps\n",
        "    eval_steps=50,  # Evaluate every 500 steps\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.5,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "    # warmup_steps=500,\n",
        "    logging_dir='./logs',\n",
        "    fp16=True,\n",
        "    # bf16=False,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1xm4zRAXZ45r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "a23a410a-dba9-4215-86b2-d9a84d9bb779"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 3268\n",
            "Sample rate: 16000\n",
            "Dataset size: 318\n",
            "Sample rate: 16000\n",
            "Number of classes: 53\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='107' max='818' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [107/818 01:57 < 13:14, 0.89 it/s, Epoch 0.26/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Hamming Score</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.288524</td>\n",
              "      <td>0.058005</td>\n",
              "      <td>0.305031</td>\n",
              "      <td>10.126400</td>\n",
              "      <td>31.403000</td>\n",
              "      <td>3.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.147000</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.044084</td>\n",
              "      <td>10.006200</td>\n",
              "      <td>31.780000</td>\n",
              "      <td>3.998000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a6129270837c>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m                     ):\n\u001b[1;32m   2488\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.evaluate()\n",
        "import torch\n",
        "torch.save(trainer.model.state_dict(), \"model_weights_1227.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "FljL-I71eOto",
        "outputId": "3d35724e-c172-4287-9038-c4c76d9abfb2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 02:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OneDrive Uploading"
      ],
      "metadata": {
        "id": "6FIK_zV4fx_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://rclone.org/install.sh | sudo bash\n",
        "!mkdir /root/.config/rclone/\n",
        "config = \"\"\"[jmc]\n",
        "type = onedrive\n",
        "client_id = 9a4d2af8-46e3-49b7-959d-0d67b733a868\n",
        "client_secret = S7F8Q~KuxCozDp69BJoWUwOZVDPURsnQpLQRpcRE\n",
        "token = {\"access_token\":\"eyJ0eXAiOiJKV1QiLCJub25jZSI6ImVOaTVYOEJFV2g4SGJ5cFdMdXFqX2d4eVh4X2RyeGtvTnlzX3gzM2hnTUUiLCJhbGciOiJSUzI1NiIsIng1dCI6Inp4ZWcyV09OcFRrd041R21lWWN1VGR0QzZKMCIsImtpZCI6Inp4ZWcyV09OcFRrd041R21lWWN1VGR0QzZKMCJ9.eyJhdWQiOiIwMDAwMDAwMy0wMDAwLTAwMDAtYzAwMC0wMDAwMDAwMDAwMDAiLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC81ODljNzZmNS1jYTE1LTQxZjktODg0Yi01NWVjMTVhMDY3MmEvIiwiaWF0IjoxNzMyODMyNDE4LCJuYmYiOjE3MzI4MzI0MTgsImV4cCI6MTczMjgzNDUxOCwiYWNjdCI6MCwiYWNyIjoiMSIsImFjcnMiOlsiYzEiXSwiYWlvIjoiQVZRQXEvOFlBQUFBQUhsYmoxRW13eWRqWjlWZk9JOERabkszT1hCRXpUamxCUUEvY2dkVkYvY0hWUVQ3K0I5NzRLU0ZXYzR1amZoK0dNRFBKdFRuSFB5UndWZlJlR0dvS2U2QjhHL3dUVGpDYU44d2tXdUE3bVU9IiwiYW1yIjpbInB3ZCIsIm1mYSJdLCJhcHBfZGlzcGxheW5hbWUiOiJyY2xvbmUiLCJhcHBpZCI6IjlhNGQyYWY4LTQ2ZTMtNDliNy05NTlkLTBkNjdiNzMzYTg2OCIsImFwcGlkYWNyIjoiMSIsImZhbWlseV9uYW1lIjoiQ2hhbiIsImdpdmVuX25hbWUiOiJKYXJlZCIsImlkdHlwIjoidXNlciIsImlwYWRkciI6IjEwOC4yNi4xNzkuMjAzIiwibmFtZSI6IkNoYW4sIEphcmVkIiwib2lkIjoiZGNhZWFiZGMtOWJhOS00MDU0LWFiMzEtYjBlMjliMDhjMGQxIiwib25wcmVtX3NpZCI6IlMtMS01LTIxLTEwMjk5ODcxNTQtMTMzMDczMzExMC0zMjY1NjkxNDctMTYwMzU3IiwicGxhdGYiOiIzIiwicHVpZCI6IjEwMDMyMDAwQzE1OTA3OTUiLCJyaCI6IjEuQVVVQTlYYWNXQlhLLVVHSVMxWHNGYUJuS2dNQUFBQUFBQUFBd0FBQUFBQUFBQUJmQWNwRkFBLiIsInNjcCI6IkZpbGVzLlJlYWQgRmlsZXMuUmVhZC5BbGwgRmlsZXMuUmVhZFdyaXRlIEZpbGVzLlJlYWRXcml0ZS5BbGwgU2l0ZXMuUmVhZC5BbGwgcHJvZmlsZSBvcGVuaWQgZW1haWwiLCJzdWIiOiJhUnpnMUp3NmJoWHg5di1VaGtXVHY4OEsycmJrbS1TYWZBaWhua0lDeUxzIiwidGVuYW50X3JlZ2lvbl9zY29wZSI6Ik5BIiwidGlkIjoiNTg5Yzc2ZjUtY2ExNS00MWY5LTg4NGItNTVlYzE1YTA2NzJhIiwidW5pcXVlX25hbWUiOiJqY2hhbjNAd3BpLmVkdSIsInVwbiI6ImpjaGFuM0B3cGkuZWR1IiwidXRpIjoicjUwYXNoSE9aRUNXWjdQM2N6MG1BQSIsInZlciI6IjEuMCIsIndpZHMiOlsiYjc5ZmJmNGQtM2VmOS00Njg5LTgxNDMtNzZiMTk0ZTg1NTA5Il0sInhtc19pZHJlbCI6IjEgMjYiLCJ4bXNfc3QiOnsic3ViIjoiTVd2MDlVNWxIN0E2UHRfM0ZaWXAxMk9LbC1ySHZRbDA2NU5ycjNNamNzNCJ9LCJ4bXNfdGNkdCI6MTQxMTQwMzMzOX0.pRiGvIdsURBnLQ3Rc885B6Dx6mlXQw7pfLGkrzalIKPVRfF-lurSf5es73XeyG81jK2BXvJ5cdUXnsA1Vu2wmsCTM5Mdbc-9p8EGn8Q5PB14CZqY6YqGHAkBa-qY-vDlj1rEptKSS_Xo_Rk83ffDVtsG_K6kdhlEw3H90ebm27Wk64bL_WupBNJxhFwV9fpBi6rQkFmxMMUeolE_usrcDRb3_Y7H0qzkzHZxeEc6voLSHXgOmMBCUBqcFWneKmzSGnO2Qh64sA5faB-dkvWF0fMaoezoMK9k_3gkvjTze80tiqKvW9NYVB_7hjtAXpRja8Pb5-ZQ3K6GehnSlVNpaQ\",\"token_type\":\"Bearer\",\"refresh_token\":\"1.AUUA9XacWBXK-UGIS1XsFaBnKvgqTZrjRrdJlZ0NZ7czqGhfAcpFAA.AgABAwEAAADW6jl31mB3T7ugrWTT8pFeAwDs_wUA9P-jXM1KD7yRqfeiTw3RiwIubrIF3Ru6z0rD0t8BwAd9WHuaeAHFaD-OcyXaBbYRJCG-RPuwzPtGv3E61da-0-2b8fMbVQkC4xftVp4OM3UIQD1wzH79bf_UnxcSYbsW05O5FsHJw8c73sNZy9ko42Ke409Mx49EdJg7Ibb6sUJlO3sKAD1k3PtidlArw8Lu-eTlF0pFEoiJMMl7tutXcZSiQSP4UoeVVOyuNAhKI47h1QVJk1NtkaD2FtriQ7vY2iCMpfUpSQLa3obbCXk9G4hIMXxAAXcN0tJTWAPm3nI3R6PTnDhBeT30mG_C2qukK1bzAGll67EzV1jCp2faSHZiPOgtviYOln46_miRFo0vLR7WyaTdBuUi3nRq49sPKS8pJuPMPCjyKTDckyx8UBUNL4nzd4oevuEcBtuC8gHnZ_cYnMCZ5FfSTWc8scV8aTtMb0is0u2I6T3BgfWiFmu2QODPp8LAsvnmvzizcxxP7fpil3_P2IgCr_v5qgXegtKcywRN50WEPebUEiLtPZzxzuBrMP_ocIBLuBN1zo_jiU9LKylkFl7T9_dQOBPUfjuZP4nct1JobZeOYPiQRCfuOFn0TlwaPBcAijL-iULjvpebvhBuYYXXsovmvfWMkhUJPdMQxQNSFwAUYNVGoScMHHRTDnPRDy2RMhco-rbcqsYEMa5eacYSO_mUDdwY51JIEH7p1Q-qwoYa3NZq0IOdr8Amke-mXEFfBsmZx_OIx_NSQbZ-1nwI9Fm3u72FFR_RpfIm5niqa7ieZrqKHeO-wIr08dR6\",\"expiry\":\"2024-11-28T17:55:17.8636308-05:00\"}\n",
        "drive_id = b!xveGKI1Ss0mbds2TAgtmMp5-fY1UTg5ImOj-tm2d6hHY3mzzWk7pQ6dT8j8Ke4Im\n",
        "drive_type = business\n",
        "\"\"\"\n",
        "with open('/root/.config/rclone/rclone.conf', 'w') as file:\n",
        "  file.write(config)"
      ],
      "metadata": {
        "id": "sqOzi5U1aung"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rclone copy /content/model_weights_1227.pth jmc:musiccaps/mert_ckpt/pth_1227 -v --stats 10s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzovTCdsbKO8",
        "outputId": "49fd3376-16d5-48a6-fc4b-8ff94abaec95"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024/12/03 21:45:02 INFO  : \n",
            "Transferred:   \t       50 MiB / 360.242 MiB, 14%, 5.556 MiB/s, ETA 55s\n",
            "Transferred:            0 / 1, 0%\n",
            "Elapsed time:        14.3s\n",
            "Transferring:\n",
            " *                        model_weights_1227.pth: 13% /360.242Mi, 5.556Mi/s, 55s\n",
            "\n",
            "2024/12/03 21:45:12 INFO  : \n",
            "Transferred:   \t      140 MiB / 360.242 MiB, 39%, 7.425 MiB/s, ETA 29s\n",
            "Transferred:            0 / 1, 0%\n",
            "Elapsed time:        24.3s\n",
            "Transferring:\n",
            " *                        model_weights_1227.pth: 38% /360.242Mi, 7.425Mi/s, 29s\n",
            "\n",
            "2024/12/03 21:45:22 INFO  : \n",
            "Transferred:   \t      240 MiB / 360.242 MiB, 67%, 8.684 MiB/s, ETA 13s\n",
            "Transferred:            0 / 1, 0%\n",
            "Elapsed time:        34.3s\n",
            "Transferring:\n",
            " *                        model_weights_1227.pth: 66% /360.242Mi, 8.684Mi/s, 13s\n",
            "\n",
            "2024/12/03 21:45:32 INFO  : \n",
            "Transferred:   \t      340 MiB / 360.242 MiB, 94%, 9.344 MiB/s, ETA 2s\n",
            "Transferred:            0 / 1, 0%\n",
            "Elapsed time:        44.3s\n",
            "Transferring:\n",
            " *                        model_weights_1227.pth: 94% /360.242Mi, 9.344Mi/s, 2s\n",
            "\n",
            "2024/12/03 21:45:38 INFO  : model_weights_1227.pth: Copied (new)\n",
            "2024/12/03 21:45:38 INFO  : \n",
            "Transferred:   \t  360.242 MiB / 360.242 MiB, 100%, 7.405 MiB/s, ETA 0s\n",
            "Transferred:            1 / 1, 100%\n",
            "Elapsed time:        50.6s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "0zNluGPGcoc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(model, dataset, batch_size=16, threshold=0.5):\n",
        "    # Create a DataLoader for batching\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for inference\n",
        "        for batch in dataloader:\n",
        "            # Get the inputs and labels from the batch\n",
        "            inputs = batch['input_values'].to(device)  # Assuming 'input_values' contains the features\n",
        "            labels = batch['labels'].cpu().numpy()  # True labels in multi-hot format\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(inputs)['logits']  # Logits of the model output\n",
        "\n",
        "            # Apply sigmoid to logits for multi-label classification\n",
        "            pred_probs = torch.sigmoid(logits).cpu().numpy()  # Get probabilities\n",
        "            # print(pred_probs)\n",
        "\n",
        "            # Apply threshold to get predicted labels\n",
        "            pred_labels = (pred_probs > threshold).astype(int)\n",
        "\n",
        "            # Store true and predicted labels\n",
        "            y_true.append(labels)\n",
        "            y_pred.append(pred_labels)\n",
        "\n",
        "    # Flatten the lists of true and predicted labels\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    # Compute classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=train_dataset.mlb.classes_))\n",
        "\n",
        "    # Compute multilabel confusion matrix\n",
        "    print(\"\\nMultilabel Confusion Matrix:\")\n",
        "    conf_matrix = multilabel_confusion_matrix(y_true, y_pred)\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return classification_report(y_true, y_pred, output_dict=True), conf_matrix\n"
      ],
      "metadata": {
        "id": "CV2VrGHwOBdt"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, pipeline\n",
        "\n",
        "# loading our model weights\n",
        "mert_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\",\n",
        "                                  trust_remote_code=True)\n",
        "# Load the saved model and processor\n",
        "model = MERTMultilabelModel(mert_model, num_classes=53)\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_state_dict(torch.load(\"model_weights_1227.pth\"), strict=False)\n",
        "model.to(device)\n",
        "\n",
        "# loading the corresponding preprocessor config\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",\n",
        "                                                     trust_remote_code=True)\n",
        "\n",
        "mlb = train_dataset.mlb\n",
        "\n",
        "test_dataset = LudwigDataset(\n",
        "    csv_path='test.csv',\n",
        "    npy_folder='/content/ludwig/npy_test',\n",
        "    processor=processor,\n",
        "    mlb=mlb,  # Use the fitted MultiLabelBinarizer\n",
        "    fit_mlb=False  # Do not refit on test data\n",
        ")\n",
        "print(test_dataset[100][\"labels\"])\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluation_results, conf_matrix = evaluate_model(model, test_dataset, batch_size=2, threshold=0.07)"
      ],
      "metadata": {
        "id": "2letsi6lcoCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538e35ec-bfb0-4f5d-d5ce-b21b7669d153"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-7aae74b44742>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model_weights_1227.pth\"), strict=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 318\n",
            "Sample rate: 24000\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "       blues---country blues       0.00      0.00      0.00         9\n",
            "      blues---electric blues       0.00      0.00      0.00         6\n",
            "         classical---baroque       0.00      0.00      0.00         2\n",
            "       classical---classical       0.00      0.00      0.00         3\n",
            "          classical---modern       0.00      0.00      0.00         3\n",
            "           classical---opera       0.00      0.00      0.00         1\n",
            "        classical---romantic       0.00      0.00      0.00         3\n",
            "        electronic---ambient       0.00      0.00      0.00        16\n",
            "          electronic---disco       0.12      0.64      0.20        28\n",
            "      electronic---downtempo       0.00      0.00      0.00        26\n",
            "    electronic---drum n bass       0.00      0.00      0.00         5\n",
            "        electronic---electro       0.18      0.19      0.19        31\n",
            "          electronic---house       0.09      0.10      0.09        31\n",
            "       electronic---new wave       0.02      0.04      0.03        28\n",
            "      electronic---synth-pop       0.16      0.98      0.27        50\n",
            "       electronic---trip hop       0.00      0.00      0.00        27\n",
            "         funk / soul---disco       0.08      0.57      0.14        21\n",
            "funk / soul---rhythm & blues       0.05      1.00      0.10        17\n",
            "          funk / soul---soul       0.11      0.19      0.14        26\n",
            "         hip hop---conscious       0.00      0.00      0.00         4\n",
            "           hip hop---gangsta       0.07      1.00      0.13        10\n",
            "      hip hop---instrumental       0.00      0.00      0.00         8\n",
            "           hip hop---pop rap       0.07      0.95      0.13        20\n",
            "              hip hop---trap       0.00      0.00      0.00         5\n",
            "          hip hop---trip hop       0.00      0.00      0.00        11\n",
            "    jazz---contemporary jazz       0.00      0.00      0.00         9\n",
            "            jazz---soul-jazz       0.00      0.00      0.00        15\n",
            "                jazz---swing       0.00      0.00      0.00         9\n",
            "              latin---cubano       0.00      0.00      0.00         1\n",
            "            latin---flamenco       0.00      0.00      0.00         3\n",
            "              latin---reggae       0.00      0.00      0.00         2\n",
            "           latin---reggaeton       0.00      0.00      0.00         0\n",
            "               latin---salsa       0.00      0.00      0.00         3\n",
            "               latin---samba       0.00      0.00      0.00         2\n",
            "                pop---ballad       0.07      1.00      0.13        22\n",
            "               pop---europop       0.00      0.00      0.00        12\n",
            "             pop---indie pop       0.00      0.00      0.00         7\n",
            "                      reggae       0.00      0.00      0.00         6\n",
            "     rock---alternative rock       0.08      0.66      0.15        29\n",
            "             rock---art rock       0.33      0.06      0.11        16\n",
            "          rock---death metal       0.00      0.00      0.00        11\n",
            "            rock---goth rock       0.00      0.00      0.00        19\n",
            "            rock---hard rock       0.03      0.25      0.05        12\n",
            "          rock---heavy metal       0.00      0.00      0.00        13\n",
            "             rock---new wave       0.10      1.00      0.19        32\n",
            "             rock---nu metal       0.00      0.00      0.00         6\n",
            "             rock---pop rock       0.12      0.66      0.20        35\n",
            "            rock---post rock       0.00      0.00      0.00        12\n",
            "            rock---post-punk       0.00      0.00      0.00        12\n",
            "            rock---prog rock       0.00      0.00      0.00        18\n",
            "                 rock---punk       0.00      0.00      0.00        17\n",
            "             rock---shoegaze       0.00      0.00      0.00        18\n",
            "         rock---viking metal       0.00      0.00      0.00         6\n",
            "\n",
            "                   micro avg       0.09      0.33      0.14       738\n",
            "                   macro avg       0.03      0.18      0.04       738\n",
            "                weighted avg       0.06      0.33      0.09       738\n",
            "                 samples avg       0.09      0.29      0.13       738\n",
            "\n",
            "\n",
            "Multilabel Confusion Matrix:\n",
            "[[[308   1]\n",
            "  [  9   0]]\n",
            "\n",
            " [[311   1]\n",
            "  [  6   0]]\n",
            "\n",
            " [[316   0]\n",
            "  [  2   0]]\n",
            "\n",
            " [[315   0]\n",
            "  [  3   0]]\n",
            "\n",
            " [[315   0]\n",
            "  [  3   0]]\n",
            "\n",
            " [[317   0]\n",
            "  [  1   0]]\n",
            "\n",
            " [[315   0]\n",
            "  [  3   0]]\n",
            "\n",
            " [[300   2]\n",
            "  [ 16   0]]\n",
            "\n",
            " [[153 137]\n",
            "  [ 10  18]]\n",
            "\n",
            " [[292   0]\n",
            "  [ 26   0]]\n",
            "\n",
            " [[307   6]\n",
            "  [  5   0]]\n",
            "\n",
            " [[260  27]\n",
            "  [ 25   6]]\n",
            "\n",
            " [[256  31]\n",
            "  [ 28   3]]\n",
            "\n",
            " [[242  48]\n",
            "  [ 27   1]]\n",
            "\n",
            " [[  1 267]\n",
            "  [  1  49]]\n",
            "\n",
            " [[289   2]\n",
            "  [ 27   0]]\n",
            "\n",
            " [[161 136]\n",
            "  [  9  12]]\n",
            "\n",
            " [[  0 301]\n",
            "  [  0  17]]\n",
            "\n",
            " [[252  40]\n",
            "  [ 21   5]]\n",
            "\n",
            " [[314   0]\n",
            "  [  4   0]]\n",
            "\n",
            " [[169 139]\n",
            "  [  0  10]]\n",
            "\n",
            " [[310   0]\n",
            "  [  8   0]]\n",
            "\n",
            " [[ 40 258]\n",
            "  [  1  19]]\n",
            "\n",
            " [[313   0]\n",
            "  [  5   0]]\n",
            "\n",
            " [[307   0]\n",
            "  [ 11   0]]\n",
            "\n",
            " [[309   0]\n",
            "  [  9   0]]\n",
            "\n",
            " [[302   1]\n",
            "  [ 15   0]]\n",
            "\n",
            " [[309   0]\n",
            "  [  9   0]]\n",
            "\n",
            " [[317   0]\n",
            "  [  1   0]]\n",
            "\n",
            " [[315   0]\n",
            "  [  3   0]]\n",
            "\n",
            " [[315   1]\n",
            "  [  2   0]]\n",
            "\n",
            " [[318   0]\n",
            "  [  0   0]]\n",
            "\n",
            " [[315   0]\n",
            "  [  3   0]]\n",
            "\n",
            " [[316   0]\n",
            "  [  2   0]]\n",
            "\n",
            " [[ 12 284]\n",
            "  [  0  22]]\n",
            "\n",
            " [[306   0]\n",
            "  [ 12   0]]\n",
            "\n",
            " [[311   0]\n",
            "  [  7   0]]\n",
            "\n",
            " [[312   0]\n",
            "  [  6   0]]\n",
            "\n",
            " [[ 78 211]\n",
            "  [ 10  19]]\n",
            "\n",
            " [[300   2]\n",
            "  [ 15   1]]\n",
            "\n",
            " [[272  35]\n",
            "  [ 11   0]]\n",
            "\n",
            " [[299   0]\n",
            "  [ 19   0]]\n",
            "\n",
            " [[189 117]\n",
            "  [  9   3]]\n",
            "\n",
            " [[302   3]\n",
            "  [ 13   0]]\n",
            "\n",
            " [[  5 281]\n",
            "  [  0  32]]\n",
            "\n",
            " [[312   0]\n",
            "  [  6   0]]\n",
            "\n",
            " [[110 173]\n",
            "  [ 12  23]]\n",
            "\n",
            " [[305   1]\n",
            "  [ 12   0]]\n",
            "\n",
            " [[306   0]\n",
            "  [ 12   0]]\n",
            "\n",
            " [[300   0]\n",
            "  [ 18   0]]\n",
            "\n",
            " [[269  32]\n",
            "  [ 17   0]]\n",
            "\n",
            " [[299   1]\n",
            "  [ 18   0]]\n",
            "\n",
            " [[311   1]\n",
            "  [  6   0]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}