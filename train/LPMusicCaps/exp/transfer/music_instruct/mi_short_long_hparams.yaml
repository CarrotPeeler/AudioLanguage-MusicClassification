# apply pretrained ckpt on short answers for finetuning on longer ones (curriculum learning)

framework: transfer

dataset:
  annotation_path: data_preprocessing/MusicIntruct/music_instruct_long.json
  data_dir: musiccaps
  inflate: false

model:
  text_type: gt
  arch: transformer
  max_length: 128
  label_smoothing: 0.1

train:
  resume: true
  # checkpoint_path: checkpoints/lp-music-caps_mi-short/mi_short_100eps.pth
  checkpoint_path: ckpt/mi_short_long_25.pth
  model_save_path: ckpt/mi_short_long
  model_save_freq: 10
  epochs: 100
  warmup_epochs: 20
  start_epoch: 0
  batch_size: 32
  world_size: 1
  lr: 0.0001
  min_lr: 1.0e-09
  print_freq: 50
  cos: true
  bart_pretrain: false
  use_early_stopping: false
  val_subset_size: 1000
  do_val: true
  precision: null # null, fp16, bf16

eval:
  results_save_path: train/LPMusicCaps/exp/transfer/music_instruct/results
  eval_sample: 64
  use_nucleus_sampling: true
  num_beams: 5

num_threads: 10
seed: 42
gpu_ids: [0] # add gpu ids here, -1 for CPU
init_method: tcp://localhost:9999 # Initialization method, includes TCP or shared file-system"
dist_backend: nccl # backend to use for distributed processing
shard_id: 0 # The shard id of current node, Starts from 0 to num_shards - 1
num_shards: 1 # Number of shards using by the job
debug_mode: false
multiprocessing_distributed: false